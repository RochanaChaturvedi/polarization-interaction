{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"32gBvjEbCI4m"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"]},{"cell_type":"code","source":["# Install requirements after mounting drive, then comment out, restart runtime (use GPU for speeding up embeddings generation)\n","%%capture\n","%cd /content/drive/My Drive/covid_tweets/artifact/code\n","!pip install -r requirements.txt"],"metadata":{"id":"ZFroc5iM-Ax-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvFvBLJV0Dkv"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import re\n","from sklearn.metrics.pairwise import cosine_similarity\n","import gc\n","import math\n","from datetime import datetime\n","from datetime import timedelta\n","from os.path import exists\n","import os\n","import glob\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","\n","from collections import Counter\n","import operator\n","import string\n","import scipy.sparse as sp\n","from multiprocessing import Pool\n","from scipy import stats as st\n","import sys\n","\n","%%capture\n","import string\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer\n","from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n","from nltk.stem import WordNetLemmatizer\n","\n","from scipy.spatial.distance import cdist\n","from sklearn.cluster import KMeans\n","import pickle\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kNg_qOe2aVCo"},"outputs":[],"source":["data_path = \"/content/drive/My Drive/covid_tweets/artifact/data/\"\n","WINDOW = 7 # outcome window\n","preTreatmentPeriod = 30 # 30 days pre event\n","interact_threshold = 0 # user considered interacting if interacting outside community more than interact_threshold fraction before the event"]},{"cell_type":"markdown","metadata":{"id":"2hYEgtfNqGuS"},"source":["# Tweet-level/daily user level computations"]},{"cell_type":"markdown","metadata":{"id":"UIttTlZLzFJ3"},"source":["## Obtain tweet embeddings on cleaned text and save"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4dBLhd70Egz"},"outputs":[],"source":["# %%capture\n","from sentence_transformers import SentenceTransformer\n","model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n","\n","def tokenize(input):\n","  return model.encode(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGav2LFe-Ong"},"outputs":[],"source":["def clean_tweet(text):\n","  temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n","  temp = re.sub(r'http\\S+', '', temp)\n","  temp = re.sub('\\s+', ' ', temp)\n","  temp = temp.strip()\n","  return temp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l79ipw3y4rB6"},"outputs":[],"source":["df =  pd.read_csv(data_path+\"tweet_text.csv\", index_col=0) # columns: ['id', 'text']\n","df = df.merge(pd.read_csv(data_path +\"tweet_level_data.csv\"), on =\"id\", how=\"inner\")\n","# Important columns in tweet_level_data.csv:\n","      #  'created_at', 'in_reply_to_user_id', 'user_id', 'id', 'valence_intensity', 'anger_intensity',\n","      #  'fear_intensity', 'sadness_intensity', 'joy_intensity',\n","      #  'user_friends_count',  'user_followers_count', 'retweet_count',\n","      #  'user_created_at', 'muslim_score', 'muslim', 'reply', 'reply_muslim']\n","userid = pd.read_csv(data_path+\"final_clean_userid.csv\") # user ids after filtering names\n","df = df[df.user_id.isin(userid.user_id)]\n","df['text'] = df['text'].apply(lambda x: clean_tweet(x))\n","df.created_at = pd.to_datetime(df['created_at'], errors='coerce')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5Xv2WaOzIxA"},"outputs":[],"source":["if not os.path.isdir(f\"{data_path}embeddings\"):\n","  os.mkdir(outpath)\n","start_date =  min(df.created_at)\n","end_date = max(df.created_at)\n","while start_date<=end_date:\n","  if exists(f\"{data_path}embeddings/tweet_embeddings_{start_date}.csv\"):\n","      start_date+=timedelta(days=1)\n","      continue\n","  print(start_date)\n","  subdata = df[df.created_at==start_date]\n","  batch_size = 10000\n","  batches = math.ceil(len(subdata)/batch_size)\n","  for i in range(1,batches+1):\n","    w = (i-1)*batch_size\n","    x = i*batch_size\n","    dt = subdata[w:x]\n","    dt.reset_index(inplace=True,drop=True)\n","    dt = pd.concat([dt,pd.DataFrame(tokenize(dt['text']))], axis = 1)\n","    del dt['text']\n","    dt.to_csv(f\"{data_path}embeddings/temp/tweet_embedding_{str(i)}.csv\", index = False)\n","  del dt\n","  gc.collect()\n","  fileList = glob.glob(f\"{data_path}embeddings/temp/tweet_embedding*.csv\")\n","  frames = []\n","  i = 1\n","  for filePath in fileList:\n","      cols = list(pd.read_csv(filePath, nrows =1))\n","      new = pd.read_csv(filePath, dtype = np.float32, usecols =[i for i in cols if i not in ['id', 'user_id', 'created_at', 'muslim']])\n","      dk = pd.read_csv(filePath, usecols = ['id', 'user_id', 'created_at', 'muslim'], dtype = object)\n","      new = dk.merge(new, left_index = True, right_index=True)\n","      i+=1\n","      try:\n","          frames.append(new)\n","      except:\n","          frames = [new]\n","      os.remove(filePath)\n","  del new, dk\n","  gc.collect()\n","\n","  data = pd.concat(frames)\n","  data.to_csv(f\"{data_path}embeddings/tweet_embeddings_{start_date}.csv\")\n","  start_date+=timedelta(days=1)"]},{"cell_type":"markdown","metadata":{"id":"2yFxa1WvrPT-"},"source":["## GCS Computation (daily user level)"]},{"cell_type":"markdown","metadata":{"id":"znqXM4aotlb-"},"source":["qi = ci/mi [ci= vector of token frequencies, normalized by mi= number of tokens in this tweet]\n","\n","ρ -i = vector of probability of tokens within group D excluding tweeter i and any token used by fewer than two people."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GlTUTdoMr3zQ"},"outputs":[],"source":["def calculate_centroids(data):\n","  CM = data[data.muslim==1].iloc[:, 0:768].mean(axis = 0) #CM: Muslim Centroid\n","  CH = data[data.muslim==0].iloc[:, 0:768].mean(axis = 0) #CH: Hindu Centroid\n","  return (CH, CM)\n","\n","def getGCS(row, CH, CM, nh, nm):\n","  q = row[0:768].values # q = vector of mean embeddings of tweeter\n","  muslim = row['muslim']\n","\n","  # adjust user's group's centroid: leaving out user i\n","  if muslim:\n","    CM = (CM*nm-q)/(nm-1)\n","  else:\n","    CH = (CH*nh-q)/(nh-1)\n","\n","\n","  m = np.linalg.norm(q-CM) # distance from Muslim centroid\n","  h = np.linalg.norm(q-CH) # distance from Hindu centroid\n","\n","  if muslim:\n","    row['gcs'] = h/(m+h)\n","  else:\n","    row['gcs'] = m/(m+h)\n","\n","  return row[['user_id', 'created_at', 'muslim', 'gcs']]\n","\n","def get_values(userids):\n","    \"\"\"\n","    Measures daily GCS for each user.\n","    returns: dataframe with daily user polarization\n","    \"\"\"\n","\n","    start_date =  pd.to_datetime(datetime.strptime(\"Jan 28 2020\", '%b %d %Y').date()).normalize()\n","    end_date =  pd.to_datetime(datetime.strptime(\"Jan 01 2021\", '%b %d %Y').date()).normalize()\n","\n","    dic = {str(x):\"mean\" for x in range(0,768)}\n","    dic.update({'user_id':'first', 'created_at' :'first'})\n","\n","    frames = []\n","    dates = []\n","    matH=[]\n","    matM=[]\n","    while start_date <= end_date:\n","        subset = pd.read_csv(f\"{data_path}embeddings/tweet_embeddings_{start_date}.csv\")\n","        subset = subset[subset.user_id.isin(userids.user_id)]\n","        del subset[\"muslim\"]\n","        if len(subset)>0:\n","          subset = subset.groupby('user_id').agg(dic)\n","          subset = subset.reset_index(drop=True)\n","          subset = subset.merge(userids, on = \"user_id\", how= \"left\", suffixes= [\"\", \"_rt\"])\n","          subset = subset[[col for col in subset.columns if not col.endswith(\"_rt\")]]\n","\n","          nh = sum(subset.muslim == 0)\n","          nm = sum(subset.muslim == 1)\n","          if nh == 0 or nm == 0:\n","            print(\"only one group on {} - nh:{} nm:{}\".format(start_date, nh, nm)) #not in this data\n","          (CH, CM) = calculate_centroids(subset)\n","          subset['GCS'] = 0.0\n","          subset['bias_comp'] = 0.0\n","\n","          sub = subset.apply(lambda row: getGCS(row, CH, CM, nh, nm), axis = 1)\n","          frames.append(sub)\n","          dates.append(start_date)\n","          matH.append(CH.to_numpy())\n","          matM.append(CM.to_numpy())\n","        else:\n","          print(\"No tweets on {}\".format(start_date)) #not in this data\n","        start_date += timedelta(days=1)\n","    data = pd.concat(frames)\n","    data = data[['user_id', 'created_at', 'muslim', 'gcs', 'bias_comp']]\n","    data.to_csv(data_path + f'/gcs.csv', index=False)\n","    return (dates, matH, matM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bJdwElj-AZj"},"outputs":[],"source":["userids = pd.read_csv(data_path+\"/final_clean_userid.csv\").reset_index(drop=True)\n","dates, matH, matM = get_values(userids)\n","df_centroids = pd.concat([pd.DataFrame(dates, columns =['created_at']),\n","                          pd.DataFrame(matM, columns = [f'H_{i}' for i in range(0,768)]),\n","                          pd.DataFrame(matM, columns = [f'M_{i}' for i in range(0,768)])], axis=1)\n","df_centroids.to_csv(data_path + f'/centroids_gcs.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"4mxD4nyz1YvJ"},"source":["## GCS BOW\n","Code adapted from: https://github.com/ddemszky/framing-twitter\n"]},{"cell_type":"markdown","source":["### utils"],"metadata":{"id":"yBxpyxRxWJ_x"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iz2lfOpNIOmz"},"outputs":[],"source":["vocab_file = data_path + 'temp_daily_vocab.txt'\n","full_vocab_file = data_path + 'full_vocab.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u5IWohI2poJ6"},"outputs":[],"source":["punct_chars = list((set(string.punctuation) | {'’', '‘', '–', '—', '~', '|', '“', '”', '…', \"'\", \"`\", '_'}) - set(['#']))\n","punct_chars.sort()\n","punctuation = ''.join(punct_chars)\n","replace = re.compile('[%s]' % re.escape(punctuation))\n","sno = nltk.stem.SnowballStemmer('english')\n","stop_words = stopwords.words('english')\n","stopwords = set(open(data_path+\"stopwords.txt\", 'r').read().splitlines())\n","stopwords = stopwords.union(stop_words) #| set(event_stopwords[event])\n","print(stopwords)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3rzLMEi1azN"},"outputs":[],"source":["#Also consider spelling and number regularization\n","def clean_text(text, keep_stopwords=False, event=None, stem=True):\n","    text = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n","    text = re.sub(r'http\\S+', '', text)\n","    text = re.sub(\"&[A-Za-z0-9_]+\",\"\", text)#remove patterns like &amp, etc.\n","    text = re.sub(\"[^a-zA-Z]\",\" \", text)\n","    text = text.lower()\n","    # eliminate urls\n","    text = re.sub(r'http\\S*|\\S*\\.com\\S*|\\S*www\\S*', ' ', text)\n","    # eliminate @mentions\n","    text = re.sub(r'\\s@\\S+', ' ', text)\n","    # substitute all other punctuation with whitespace\n","    text = replace.sub(' ', text)\n","    # replace all whitespace with a single space\n","    text = re.sub('\\s+', ' ', text)\n","    # strip off spaces on either end\n","    text = text.strip()\n","    words = [word for word in nltk.word_tokenize(text)]\n","    if not keep_stopwords:\n","        words = [w for w in words if w not in stopwords]\n","    if stem:\n","        words = [sno.stem(w) for w in words]\n","    return words\n","\n","def build_vocab(corpus):\n","    freq = {}\n","    for sent in corpus:\n","        prev = ''\n","        count = 0\n","        for w in sent:\n","            if w in freq:\n","                freq[w] += 1\n","            else:\n","                freq[w] = 1\n","            if count > 0:\n","                bigram = prev + ' ' + w\n","                if bigram in freq:\n","                    freq[bigram] += 1\n","                else:\n","                    freq[bigram] = 1\n","            count += 1\n","            prev = w\n","    vocab = [k for k,v in sorted(freq.items(), key=operator.itemgetter(1), reverse=True)]# if v > cutoff]\n","    return vocab\n","\n","def get_user_token_counts(tweets, vocab):\n","    users = tweets.groupby('user_id')\n","    user_ids =[]\n","    row_idx = []\n","    col_idx = []\n","    data = []\n","    for group_idx, (u, group), in enumerate(users):\n","        word_indices = []\n","        user_ids.append(u)\n","        for split in group['clean_text']:\n","            count = 0\n","            prev = ''\n","            for w in split:\n","                if w == '':\n","                    continue\n","                if w in vocab:\n","                    word_indices.append(vocab[w])\n","                if count > 0:\n","                    bigram = prev + ' ' + w\n","                    if bigram in vocab:\n","                        word_indices.append(vocab[bigram])\n","                count += 1\n","                prev = w\n","        for k, v in Counter(word_indices).items():\n","            col_idx.append(group_idx)\n","            row_idx.append(k)\n","            data.append(v)\n","    return (sp.csr_matrix((data, (col_idx, row_idx)), shape=(len(users), len(vocab))), np.array(user_ids))\n","\n","def split_religion(data):\n","  return data[data['muslim'] == 0], data[data['muslim'] == 1]\n","\n","def get_community_q(community_counts, exclude_user_id = None):\n","    user_sum = community_counts.sum(axis=0)\n","    if exclude_user_id:\n","        user_sum -= community_counts[exclude_user_id, :]\n","    total_sum = user_sum.sum()\n","    return user_sum / total_sum\n","\n","def get_token_user_counts(community_counts):\n","    no_tokens = community_counts.shape[1]\n","    nonzero = sp.find(community_counts)[:2]\n","    user_t_counts = Counter(nonzero[1])  # number of users using each term\n","    community_t = np.ones(no_tokens)  # add one smoothing\n","    for k, v in user_t_counts.items():\n","        community_t[k] += v\n","    return community_t\n","\n","def get_rho(H_q, M_q):\n","    return (M_q / (H_q + M_q)).transpose()\n","\n","def mutual_information(H_t, M_t, H_not_t, M_not_t, H_no, M_no):\n","    no_users = H_no + M_no\n","    all_t = H_t + M_t\n","    all_not_t = no_users - all_t + 4\n","    mi_H_t = H_t * np.log2(no_users * (H_t / (all_t * H_no)))\n","    mi_H_not_t = H_not_t * np.log2(no_users * (H_not_t / (all_not_t * H_no)))\n","    mi_M_t = M_t * np.log2(no_users * (M_t / (all_t * M_no)))\n","    mi_M_not_t = M_not_t * np.log2(no_users * (M_not_t / (all_not_t * M_no)))\n","    return (1 / no_users * (mi_H_t + mi_H_not_t + mi_M_t + mi_M_not_t)).transpose()[:, np.newaxis]\n","\n","def chi_square(H_t, M_t, H_not_t, M_not_t, H_no, M_no):\n","    no_users = H_no + M_no\n","    all_t = H_t + M_t\n","    all_not_t = no_users - all_t + 4\n","    chi_enum = no_users * (H_t * M_not_t - H_not_t * M_t) ** 2\n","    chi_denom = all_t * all_not_t * (H_t + H_not_t) * (M_t + M_not_t)\n","    return (chi_enum / chi_denom).transpose()[:, np.newaxis]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"paGtJ6GT1pYq"},"outputs":[],"source":["def calculate_polarization(H_counts, M_counts, leaveout=True):\n","    H_user_total = H_counts.sum(axis=1)\n","    M_user_total = M_counts.sum(axis=1)\n","\n","    H_user_distr = (sp.diags(1 / H_user_total.A.ravel())).dot(H_counts)  # get row-wise distributions\n","    M_user_distr = (sp.diags(1 / M_user_total.A.ravel())).dot(M_counts)\n","    H_no = H_counts.shape[0]\n","    M_no = M_counts.shape[0]\n","    assert (set(H_user_total.nonzero()[0]) == set(range(H_no)))  # make sure there are no zero rows\n","    assert (set(M_user_total.nonzero()[0]) == set(range(M_no)))  # make sure there are no zero rows\n","\n","    H_q = get_community_q(H_counts)\n","    M_q = get_community_q(M_counts)\n","    H_t = get_token_user_counts(H_counts)\n","    M_t = get_token_user_counts(M_counts)\n","    H_not_t = H_no - H_t + 2  # because of add one smoothing\n","    M_not_t = M_no - M_t + 2  # because of add one smoothing\n","    func = chi_square\n","\n","    # apply measures via leave-out\n","    H_addup = 0\n","    M_addup = 0\n","    H_leaveout_no = H_no - 1\n","    M_leaveout_no = M_no - 1\n","    H_Leaveouts = []\n","    M_Leaveouts = []\n","    for i in range(H_no):\n","        H_leaveout_q = get_community_q(H_counts, i)\n","        token_scores_H = 1. - get_rho(H_leaveout_q, M_q)\n","        user_measure= H_user_distr[i, :].dot(token_scores_H)[0, 0]\n","        H_Leaveouts.append(user_measure)\n","        H_addup += user_measure\n","    for i in range(M_no):\n","        M_leaveout_q = get_community_q(M_counts, i)\n","        token_scores_M = get_rho(H_q, M_leaveout_q)\n","        user_measure = M_user_distr[i, :].dot(token_scores_M)[0, 0]\n","        M_Leaveouts.append(user_measure)\n","        M_addup += user_measure\n","\n","    M_val = 1 / M_no * M_addup\n","    H_val = 1 / H_no * H_addup\n","    return (1/2 * (H_val + M_val), H_Leaveouts, M_Leaveouts)\n","\n","def get_values(data, start_date):\n","    \"\"\"\n","    Measure polarization.\n","    :param data: dataframe with 'text' and 'user_id'\n","    :return:\n","    \"\"\"\n","\n","    H_tweets, M_tweets = split_religion(data)  # get community tweets\n","\n","    # get vocab\n","    vocab = {w: i for i, w in\n","             enumerate(open(vocab_file, 'r').read().splitlines())}\n","\n","    (H_counts, H_user_ids) = get_user_token_counts(H_tweets, vocab)\n","    (M_counts, M_user_ids) = get_user_token_counts(M_tweets, vocab)\n","\n","    del H_tweets\n","    del M_tweets\n","    gc.collect()\n","\n","    H_user_len = H_counts.shape[0]\n","    M_user_len = M_counts.shape[0]\n","\n","    all_counts = sp.vstack([H_counts, M_counts])\n","\n","    wordcounts = all_counts.nonzero()[1]\n","\n","    # filter words used by fewer than 2 people\n","    all_counts = all_counts[:, np.array([(np.count_nonzero(wordcounts == i) > 1) for i in range(all_counts.shape[1])])]\n","\n","    H_counts = all_counts[:H_user_len, :]\n","    M_counts = all_counts[H_user_len:, :]\n","    del wordcounts\n","    del all_counts\n","    gc.collect()\n","\n","    # filter users who did not use words from vocab\n","    H_nonzero = set(H_counts.nonzero()[0])\n","    M_nonzero = set(M_counts.nonzero()[0])\n","    H_filter_idx = np.array([(i in H_nonzero) for i in range(H_counts.shape[0])])\n","    M_filter_idx = np.array([(i in M_nonzero) for i in range(M_counts.shape[0])])\n","    H_counts = H_counts[H_filter_idx, :]  # filter users who did not use words from vocab\n","    M_counts = M_counts[M_filter_idx, :]\n","    H_user_ids = list(H_user_ids[H_filter_idx])\n","    M_user_ids = list(M_user_ids[M_filter_idx])\n","    del H_nonzero\n","    del M_nonzero\n","    gc.collect()\n","\n","    (actual_val, H_Leaveouts, M_Leaveouts) = calculate_polarization(H_counts, M_counts)\n","    df = pd.DataFrame({'leaveout_score': H_Leaveouts + M_Leaveouts,\n","                       'user_id': H_user_ids + M_user_ids})\n","    df[\"date\"] = start_date\n","    df[\"day_estimate\"] = actual_val\n","    del H_counts\n","    del M_counts\n","    gc.collect()\n","    sys.stdout.flush()\n","    actual_val = \"{:.2f}\".format(actual_val)\n","    total_users = H_user_len + M_user_len\n","    print(f\"Actual value:{actual_val}, Number of users:{total_users}, % of Muslims:{M_user_len*100/total_users}\")\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"UCrzdaYZ2C6z"},"source":["### user-daily aggregate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyTDNvii595A"},"outputs":[],"source":["df =  pd.read_csv(data_path+\"tweet_text.csv\", index_col=0)\n","df = df.merge(pd.read_csv(f\"{data_path}/tweet_level_data.csv\"), on =\"id\", how=\"inner\")\n","userid = pd.read_csv(data_path+\"final_clean_userid.csv\")\n","df = df[df.user_id.isin(userid.user_id)]\n","df.created_at = pd.to_datetime(df['created_at'], errors='coerce')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nmm9g2QUYeIi"},"outputs":[],"source":["start_date =  pd.to_datetime(datetime.strptime(\"Jan 28 2020\", '%b %d %Y').date()).normalize()\n","end_date =  pd.to_datetime(datetime.strptime(\"Jan 01 2021\", '%b %d %Y').date()).normalize()\n","full_vocab = set()\n","\n","while start_date <= end_date:\n","    daily_data =  df[df.created_at==start_date]\n","\n","    if len(daily_data)>0 :\n","      p = Pool(3)\n","      daily_data['clean_text'] = p.map(clean_text, daily_data['text'])\n","      p.close()\n","      p.join()\n","      daily_vocab = build_vocab(daily_data.clean_text)\n","      with open(vocab_file, 'w') as f:\n","        f.write('\\n'.join(daily_vocab))\n","\n","      full_vocab = set(daily_vocab).union(full_vocab)\n","      data = get_values(daily_data, start_date)\n","      try:\n","        full_data = pd.read_pickle(data_path + \"estimates.pkl\")\n","        full_data = pd.concat([full_data, data], ignore_index=True)\n","      except (OSError, IOError) as e:\n","        print(f\"error: {e}\")\n","        full_data = data\n","      full_data.to_pickle(data_path+\"estimates.pkl\")\n","\n","    start_date += timedelta(days=1)\n","with open(full_vocab_file, 'w') as f:\n","        f.write('\\n'.join(full_vocab))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTXXbJSP2Oj0"},"outputs":[],"source":["# Save BOW GCS to CSV\n","df = pd.read_pickle(data_path+\"estimates.pkl\")\n","df['bowgcs'] = df['leaveout_score']\n","del df['leaveout_score']\n","df.to_csv(data_path+\"bow_gcs.csv\", index = False)"]},{"cell_type":"markdown","metadata":{"id":"E4B-OxrwqhDQ"},"source":["# User-event level computations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fXdLALNgshW"},"outputs":[],"source":["def geteventDate(EVENT):\n","  if EVENT == \"juntaCurfew\":\n","    EVENT_DATE = 'Mar 22 2020'\n","  elif EVENT == \"tabliqi\":\n","    EVENT_DATE = 'Mar 31 2020'\n","  elif EVENT==\"migrantraildeath\":\n","    EVENT_DATE=\"May 8 2020\"\n","  elif EVENT == \"Coronil\":\n","    EVENT_DATE = \"Jun 23 2020\"\n","  elif EVENT==\"exam\":\n","    EVENT_DATE = \"Aug 23 2020\"\n","  elif EVENT == \"gdpcontracts\":\n","    EVENT_DATE = \"Aug 31 2020\"\n","  elif EVENT==\"BiharManifesto\":\n","    EVENT_DATE=\"Oct 22 2020\"\n","\n","  EVENT_DATE = datetime.strptime(EVENT_DATE, '%b %d %Y').date()\n","  return pd.to_datetime(EVENT_DATE).normalize()\n","\n","EVENTS = [\"juntaCurfew\"\n","          , \"tabliqi\"\n","          , \"migrantraildeath\"\n","          , \"Coronil\"\n","          , \"exam\"\n","          , \"gdpcontracts\"\n","          , \"BiharManifesto\"\n","          ]"]},{"cell_type":"markdown","metadata":{"id":"4OqC2qsaAYS3"},"source":["## Compute Treatment Variable: interact\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fi5PXIRiQ8rO"},"outputs":[],"source":["df = pd.read_csv(data_path+\"/tweet_level_data.csv\")\n","df = df[~df.reply_muslim.isna()][[\"id\", \"user_id\", \"created_at\", \"muslim\", \"in_reply_to_user_id\", \"reply_muslim\"]]\n","df.in_reply_to_user_id = df.in_reply_to_user_id.astype(int)\n","df.muslim=df.muslim.astype(int)\n","df.reply_muslim = df.reply_muslim.astype(int)\n","df.created_at = pd.to_datetime(df['created_at'], errors='coerce')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYsNM0LVj8VC"},"outputs":[],"source":["for event in EVENTS:\n","  print(event)\n","  eventdate = geteventDate(event)\n","  d = df[df.created_at< eventdate]\n","\n","  dic = {'reply_muslim': 'sum', 'muslim':'first', 'id':'size'}\n","  subdata = d.groupby(\"user_id\").agg(dic).reset_index()\n","  subdata['frac_muslim_reply'] = subdata['reply_muslim']/subdata['id']\n","  subdata['interact'] = subdata.apply(lambda row: 1-row['frac_muslim_reply'] if row['muslim'] else row['frac_muslim_reply'], axis=1)\n","\n","  subdata.interact = subdata.interact.apply(lambda x: 0 if x<interact_threshold else 1).astype(int)\n","  print(f\"Total users:{len(subdata)}   interacts:{len(subdata[subdata.interact==1])}   non-interacts:{len(subdata[subdata.interact==0])}\")\n","  print(\"\\n\\n\")\n","  subdata.to_csv(data_path+f\"/interact_before_{event}.csv\")"]},{"cell_type":"markdown","metadata":{"id":"KKBHoyQb4WOL"},"source":["## Compute Outcome (1): GCS-diff around event window"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_XAOjjhESi0"},"outputs":[],"source":["df = pd.read_csv(f'{data_path}/gcs.csv')[['user_id',  'created_at', 'gcs']]\n","df.created_at = pd.to_datetime(df['created_at'], errors='coerce')\n","d = {'user_id':'first', 'gcs':'mean'}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmFkJwec4cMA"},"outputs":[],"source":["for EVENT in EVENTS:\n","  print(EVENT)\n","  eventdate = geteventDate(EVENT)\n","  df[\"diff_days\"] = df['created_at'].apply(lambda x: (eventdate - x).days)\n","\n","  subdata = df[abs(df.diff_days)<=WINDOW]\n","  subdata = subdata[subdata.diff_days>-WINDOW]\n","\n","  post_data = subdata[subdata.diff_days<=0]\n","  pre_data = subdata[subdata.diff_days>0]\n","\n","  #avg gcs in post data\n","  post_data = post_data.groupby('user_id').agg(d)\n","  post_data.reset_index(drop = True, inplace = True)\n","  print(f\"post users:{len(post_data)}\")\n","\n","  #avg gcs in pre data\n","  pre_data = pre_data.groupby('user_id').agg(d)\n","  pre_data.reset_index(drop = True, inplace = True)\n","  print(f\"pre users:{len(pre_data)}\")\n","\n","  subdata = pd.merge(post_data, pre_data, on = [\"user_id\"], suffixes=('', '_pre'))\n","\n","  subdata[\"gcs_diff\"] = subdata[\"gcs\"] - subdata[\"gcs_pre\"]\n","  del subdata[\"gcs\"]\n","  del subdata[\"gcs_pre\"]\n","  subdata.to_csv(f\"{data_path}/{EVENT}gcsdiff_{WINDOW}.csv\")\n","  print(len(subdata))\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"p16Gfa7L0kz0"},"source":["## Combine with pretreatment covariates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTD54YS9Jt4z"},"outputs":[],"source":["def gettweetdata(eventdate):\n","  df = pd.read_csv(f\"{data_path}/tweet_level_data.csv\")\n","  df.created_at = pd.to_datetime(df['created_at'], errors='coerce')\n","  df['user_created_at'] = pd.to_datetime(df['user_created_at'], errors='coerce')\n","  df = df[['valence_intensity', 'anger_intensity', 'fear_intensity',\n","        'sadness_intensity', 'joy_intensity',\n","        'user_friends_count', 'user_followers_count',\n","        'retweet_count', 'user_created_at',\n","        'muslim_score', 'reply', 'muslim', 'user_id', \"id\",\n","        'created_at']]\n","  df['valence_intensity'] = df.valence_intensity.astype(np.float32)\n","  df['anger_intensity'] = df.anger_intensity.astype(np.float32)\n","  df['fear_intensity'] = df.fear_intensity.astype(np.float32)\n","  df['sadness_intensity'] = df.sadness_intensity.astype(np.float32)\n","  df['joy_intensity'] = df.joy_intensity.astype(np.float32)\n","  df['muslim_score'] = df.muslim_score.astype(np.float32)\n","  df['muslim'] = df.muslim.astype(np.int8)\n","  df['reply'] = df.reply.astype(np.int16)\n","  return df[(df.created_at >= eventdate - timedelta(preTreatmentPeriod))&(df.created_at<eventdate)]\n","\n","gcsdata = pd.read_csv(f'{data_path}/gcs.csv')[['user_id',  'created_at', 'gcs']]\n","gcsdata.created_at = pd.to_datetime(gcsdata['created_at'], errors='coerce')\n","gcsdata['gcs'] = gcsdata['gcs'].astype(np.float32)\n","dic = {'valence_intensity':\"mean\", 'anger_intensity':\"mean\", 'fear_intensity':\"mean\",\n","       'sadness_intensity':\"mean\", 'joy_intensity':\"mean\",\n","       'user_friends_count':'last', 'user_followers_count':'last',\n","       'retweet_count':'mean', 'user_created_at':'first',\n","       'muslim_score':'first', 'reply':'mean', 'muslim':'first', 'user_id':'first', \"id\":'size'}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_vNVZpJBVXh"},"outputs":[],"source":["for EVENT in EVENTS:\n","  print(EVENT)\n","  eventdate = geteventDate(EVENT)\n","\n","  # load gcs_diff\n","  gcs_diff_data = pd.read_csv(f\"{data_path}/{EVENT}gcsdiff_{WINDOW}.csv\", index_col = 0)\n","\n","  # combine with treatment variable\n","  interactdata= pd.read_csv(data_path+f\"/interact_before_{EVENT}.csv\", index_col =0)\n","  subdata = gcs_diff_data.merge(interactdata, on = \"user_id\", how =\"inner\")\n","  del interactdata, gcs_diff_data\n","\n","  # combine aggregates pretreatmentPeriod (30 days pre-event)\n","  predata = gettweetdata(eventdate)\n","  predata = predata[predata.user_id.isin(subdata.user_id)]\n","  predata = predata.groupby('user_id').agg(dic)\n","  predata['tweet_frequency'] = predata['id']\n","\n","  predata.reset_index(drop=True, inplace =True)\n","\n","  eventgcsdata = gcsdata[(gcsdata.created_at >= eventdate - timedelta(preTreatmentPeriod))&(gcsdata.created_at<eventdate)]\n","  eventgcsdata = eventgcsdata[eventgcsdata.user_id.isin(subdata.user_id)]\n","  eventgcsdata = eventgcsdata.groupby('user_id').agg({\"user_id\":\"first\", 'gcs':\"mean\"})\n","  eventgcsdata.reset_index(drop=True, inplace =True)\n","  subdata = subdata.merge(eventgcsdata, on = \"user_id\", how = \"inner\", suffixes = ['', '_y'])\n","  subdata = subdata[[col for col in subdata.columns if not col.endswith(\"_y\")]]\n","\n","  subdata = subdata.merge(predata, on = \"user_id\", how = \"inner\", suffixes = ['', '_y'])\n","  subdata = subdata[[col for col in subdata.columns if not col.endswith(\"_y\")]]\n","  del subdata['id']\n","  gc.collect()\n","\n","  subdata.to_csv(f\"{data_path}/causal_data_{EVENT}_{str(WINDOW)}_allcovariates_except_topic.csv\")"]},{"cell_type":"markdown","metadata":{"id":"CaTuot172gCj"},"source":["## Topic modeling"]},{"cell_type":"markdown","metadata":{"id":"x1f5cdeHwWaV"},"source":["### Utility Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQTbQqIf-1MY"},"outputs":[],"source":["wordnet_lemmatizer = WordNetLemmatizer()\n","stop_words = stopwords.words('english')\n","stop_words = set(stop_words + [\"tweet\", \"twitter\", \"sir\", \"tweets\", \"ji\", \"like\", \"ki\", \"ke\", \"ko\", \"ka\", \"hai\",\n","                               \"se\", \"aur\", \"bhi\", \"na\", \"nahi\", \"let\", \"say\", \"yes\", \"no\", \"think\", \"know\", \"ur\",\n","                               \"going\", \"kya\", \"guy\", \"due\", \"man\", \"got\",\n","                               'ye', 'bhai', 'kuch','toh', 'ho', 'jo', 'tha', 'hoga', 'mein', 'hi', 'hain', 'kar',\n","                               'lo', 'ek', 'ya', 'gaya', 'kal', 'koi', 'ab', 'nhi', 'yeh', 'said', 'say', 'use'])\n","\n","\n","def clean_tweet(text, labeling=True, hide_covid=False):\n","  temp = text.lower()\n","  temp = re.sub(\"(\\bcorona\\b)|(\\bncov\\b)|(covid-19)|(\\bcovid\\b)\" , \"coronavirus\", temp)\n","  if hide_covid:\n","    temp=re.sub(\"(coronavirus)|(corona)\", \" \", temp)\n","  temp = re.sub(\"(pfizer)|(moderna)|(janseen)|(sinova)|(sinopharm)|(sputnik)|(covishield)|(covaxin)|(coronavac)|(sinovac)|(novavax)\",\"vaccine\", temp)\n","  temp = re.sub(\"(astrazeneca)|(johnson)|(johnson\\s*(&|and)\\s*johnson)|(j&j)|(serum)|(biontech)|(sinopharm)|(covishield)|(covaxin)\",\"vaccine\", temp)\n","\n","  temp = re.sub(\"@[a-z0-9_]+\",\"\", temp)\n","  temp =  temp.replace(\"_\", \" \")\n","  temp = re.sub(r'&amp', '&', temp)\n","  temp = re.sub(r'&quot', '\"', temp)\n","  temp = re.sub(r'&lt', '<', temp)\n","  temp = re.sub(r'&gt', '>', temp)\n","  temp = re.sub(r'http\\S+', '', temp)\n","  temp = re.sub(\"[^a-z]\",\" \", temp)\n","  temp = \"\".join([char for char in temp if char not in string.punctuation])\n","  temp = re.sub('\\s+', ' ', temp)\n","  temp = temp.strip()\n","  temp = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in\n","                   nltk.word_tokenize(temp) if word not in stop_words])\n","  return temp\n","\n","\n","def c_tf_idf(documents, m, ngram_range=(1, 1)):\n","    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n","    t = count.transform(documents).toarray()\n","    w = t.sum(axis=1)\n","    tf = np.divide(t.T, w)\n","    sum_t = t.sum(axis=0)#total count of word across corpus\n","    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n","    tf_idf = np.multiply(tf, idf)\n","    return tf_idf, count\n","\n","def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=50):\n","    words = count.get_feature_names_out()\n","    labels = list(docs_per_topic.topic)\n","    tf_idf_transposed = tf_idf.T\n","    indices = tf_idf_transposed.argsort()[:, -n:]\n","    top_n_words = {label: [(words[j], \"{:.3f}\".format(tf_idf_transposed[i][j])) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n","    return top_n_words\n","\n","def extract_topic_sizes(df):\n","    topic_sizes = (df.groupby(['topic'])\n","                     .bi_text\n","                     .count()\n","                     .reset_index()\n","                     .rename({\"topic\": \"Topic\", \"bi_text\": \"Size\"}, axis='columns')\n","                     .sort_values(\"Size\", ascending=False))\n","    return topic_sizes\n","\n","def bigram_pasting(sentence):\n","  return \" \".join(phrase_model[sentence.split(\" \")])"]},{"cell_type":"markdown","source":["### data prep for topic modeling"],"metadata":{"id":"ptIZtdTZLHXq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hQ_byx-JZR3"},"outputs":[],"source":["tweets_df =  pd.read_csv(data_path+\"tweet_text.csv\", index_col=0) #read tweet_text.csv\n","tweets_df = tweets_df.drop_duplicates(subset=['text'])\n","tweets_df = tweets_df.merge(pd.read_csv(data_path +\"/tweet_level_data.csv\"), on =\"id\", how=\"inner\", suffixes= [\"\", \"_y\"])[['id','text','created_at','user_id']]\n","tweets_df = tweets_df.drop(columns=[i for i in tweets_df.columns if i.endswith(\"_y\")])\n","tweets_df.created_at = pd.to_datetime(tweets_df['created_at'], errors='coerce')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Go0Ak0rrZsbE"},"outputs":[],"source":["all_tweets = pd.DataFrame()\n","for EVENT in EVENTS:\n","  eventdate = geteventDate(EVENT)\n","  users = pd.read_csv(f\"{data_path}/causal_data_{EVENT}_{str(WINDOW)}_allcovariates_except_topic.csv\", index_col=0)['user_id']\n","  tweets = tweets_df[(tweets_df.created_at >= eventdate - timedelta(preTreatmentPeriod))&(tweets_df.created_at<eventdate+timedelta(WINDOW))]\n","  tweets = tweets[tweets.user_id.isin(users)]\n","\n","  start_date = eventdate - timedelta(preTreatmentPeriod)\n","  end_date =  eventdate+timedelta(WINDOW)\n","  embdata = []\n","  while start_date < end_date:\n","    emb = pd.read_csv(f\"{data_path}embeddings/tweet_embeddings_{start_date}.csv\", index_col=0)\n","    emb = emb.astype({str(col): 'float16' for col in range(0,768)})\n","    embdata.append(emb)\n","    start_date += timedelta(days=1)\n","  embdata = pd.concat(embdata)\n","  del embdata['muslim']\n","  embdata = embdata[embdata.user_id.isin(users)]\n","  tweets = tweets.merge(embdata, on =\"id\", how=\"inner\", suffixes= [\"\", \"_y\"])\n","  tweets = tweets.drop(columns=[i for i in tweets.columns if i.endswith(\"_y\")])\n","  tweets['text'] = tweets['text'].apply(lambda x: clean_tweet(x, labeling=False))\n","\n","  tweets = tweets[~tweets.text.isna()]\n","  if len(all_tweets)>0:\n","    all_tweets = pd.concat([all_tweets, tweets]).drop_duplicates()\n","    all_tweets = all_tweets.drop_duplicates(subset=['text'])\n","  else:\n","    all_tweets = tweets\n","  del embdata, users\n","  gc.collect()\n","\n","tweets = all_tweets\n","tweets = tweets[tweets.text!='']\n","tweets = tweets[~tweets['text'].isna()]\n","\n","del all_tweets\n","gc.collect()\n","tweets.to_csv(data_path + '/tweets_topics_df.csv')"]},{"cell_type":"markdown","metadata":{"id":"17SpLvezfQh_"},"source":["### Finding elbow"]},{"cell_type":"code","source":["tweets= pd.read_csv(data_path + '/tweets_topics_df.csv', index_col=0)\n","x = tweets[[str(i) for i in range(0,768)]].values\n","del tweets\n","gc.collect()"],"metadata":{"id":"DuJB3D8XFVLj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gH2WxSIccojm"},"outputs":[],"source":["inertias = []\n","total_clusters = []\n","for k in range(3,11,1):\n","  # Building and fitting the model\n","  kmeans = KMeans(n_clusters = k, random_state=23)\n","  kmeans.fit(x)\n","  with open(f'{data_path}kmeans{str(k)}_7events.pickle', 'wb') as handle:\n","    pickle.dump(kmeans, file=handle,protocol=None, fix_imports=True)\n","  inertias.append(kmeans.inertia_)\n","  total_clusters.append(k)\n","  print(f\"{str(k)} done\")\n","plt.plot(total_clusters, inertias, 'bx-')\n","plt.xlabel('Number of Clusters')\n","plt.ylabel('Inertia')\n","plt.title('Elbow Curve: Inertia vs. Clusters')\n","plt.show()"]},{"cell_type":"markdown","source":["### Predict"],"metadata":{"id":"C3Ld2dZ8SWs5"}},{"cell_type":"code","source":["tweets= pd.read_csv(data_path + '/tweets_topics_df.csv', index_col=0)\n","n = 7\n","with open(f'{data_path}kmeans{str(n)}_7events.pickle', 'rb') as handle:\n","    kmeans = pickle.load(file=handle)\n","tweets[\"topic\"] = kmeans.predict(tweets[[str(i) for i in range(0,768)]].values)\n","tweets.to_csv(data_path+f'/kmeans_{n}_window_{WINDOW}_7events.csv')\n","del tweets[\"text\"], tweets[\"user_id\"], tweets[\"created_at\"]\n","gc.collect()"],"metadata":{"id":"UVjbeYCNaXFs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# interpret topic labels\n","ntopics = 7\n","topic_data = pd.read_csv(data_path+f'/kmeans_{ntopics}_window_{WINDOW}_7events.csv', index_col =0).reset_index(drop=True)\n","topic_data.set_index(['id'], drop=False, append=False, inplace = True)\n","topic_data = topic_data[[col for col in topic_data.columns if col not in [str(i) for i in range(0,768)]]]\n","gc.collect()\n","\n","topic_data[\"text\"] = topic_data[\"text\"].map(lambda tweet: clean_tweet(tweet, labeling=True, hide_covid= True))\n","phrase_model = Phrases([doc.split(\" \") for doc in topic_data['text']],\n","                      connector_words=ENGLISH_CONNECTOR_WORDS, scoring = 'npmi', threshold=-1)\n","p = Pool(2)\n","topic_data['bi_text'] = p.map(bigram_pasting, topic_data['text'])\n","p.close()\n","p.join()\n","\n","# join docs\n","docs_per_topic = topic_data.groupby(['topic'], as_index = False).agg({'bi_text': ' '.join})\n","tf_idf, count = c_tf_idf(docs_per_topic.bi_text.values, m=len(topic_data))\n","top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=50)\n","\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","topic_label = pd.DataFrame(top_n_words)\n","topic_label.to_csv(data_path + f'/labels_7topics_7events_window_{WINDOW}.csv', index = 0)"],"metadata":{"id":"WbaxuXIIMBnR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Quantitative cluster analysis\n"],"metadata":{"id":"SUzErh3vK_Kk"}},{"cell_type":"code","source":["data = pd.DataFrame()\n","n=7 # check for n clusters/topics\n","with open(f'{data_path}kmeans{str(n)}_7events.pickle', 'rb') as handle:\n","    kmeans = pickle.load(file=handle)\n","labels = kmeans.labels_\n","centroids = kmeans.cluster_centers_\n","# Calculate the distance between centroids using cosine distance\n","distances = cdist(centroids, centroids, 'cosine')\n","print(labels)\n","print(distances)\n","print()\n","\n","# Calculate the distance between centroids using Euclidean distance\n","distances = cdist(centroids, centroids, 'euclidean')\n","print(np.around(distances, 2))\n","print(labels)\n","print(distances)\n","print()\n","\n","for i in range(0,n):\n","  for j in range(i, n):\n","    tweets = pd.read_csv(data_path+f'/kmeans_{n}_window_{WINDOW}_7events.csv', index_col=0)\n","    tweets = tweets.astype({str(col): 'float16' for col in range(0,768)})\n","    x = tweets[tweets['topic'] == i][[str(i) for i in range(0,768)]].values\n","    y = tweets[tweets['topic'] == j][[str(i) for i in range(0,768)]].values\n","    del tweets\n","    gc.collect()\n","\n","    cos = np.mean(cosine_similarity(x,y))\n","    print(cos.shape)\n","    del x,y\n","    gc.collect()\n","\n","    temp = pd.DataFrame([[i,j,cos]], columns=['cluster_source', 'cluster_target','mean_cosine'])\n","\n","    data = data.append(temp)\n","\n","data.to_csv(data_path+f'/pairwise_avg_cluster_distances_kmeans_{n}_window_{WINDOW}_7events.csv')\n","print(data)"],"metadata":{"id":"UxveeILQ0Cv7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Merge topics after examining labels, clusters"],"metadata":{"id":"aV9UEjDO5D1x"}},{"cell_type":"code","source":["merge = True\n","ntopics=7\n","topic_data = pd.read_csv(data_path+f'/kmeans_{ntopics}_window_{WINDOW}_7events.csv', index_col =0).reset_index(drop=True)\n","topic_data.set_index(['id'], drop=False, append=False, inplace = True)\n","topic_data = topic_data[[col for col in topic_data.columns if col not in [str(i) for i in range(0,768)]]]\n","gc.collect()\n","if merge==True:\n","  topic_data.topic = topic_data.topic.map(lambda t: 0 if t in [0,2,3,4] else 1 if t==1 else 2 if t==5 else 3)# covid, pol-rel, china, economy"],"metadata":{"id":"7eQhpSuYfvM1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### label merged version"],"metadata":{"id":"wBF8XBe641xS"}},{"cell_type":"code","source":["#interpret\n","ntopics = 4\n","topic_data[\"text\"] = topic_data[\"text\"].map(lambda tweet: clean_tweet(tweet, labeling=True, hide_covid= True))\n","phrase_model = Phrases([doc.split(\" \") for doc in topic_data['text']],\n","                      connector_words=ENGLISH_CONNECTOR_WORDS, scoring = 'npmi', threshold=-1)\n","p = Pool(2)\n","topic_data['bi_text'] = p.map(bigram_pasting, topic_data['text'])\n","p.close()\n","p.join()\n","\n","# join docs\n","docs_per_topic = topic_data.groupby(['topic'], as_index = False).agg({'bi_text': ' '.join})\n","tf_idf, count = c_tf_idf(docs_per_topic.bi_text.values, m=len(topic_data))\n","top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=50)\n","\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","topic_label = pd.DataFrame(top_n_words)\n","topic_label.to_csv(data_path + f'/merged_labels_7to4_kmeans_7events_window_{WINDOW}.csv', index = 0)"],"metadata":{"id":"2t8KXIVy44Ou"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### pivot"],"metadata":{"id":"KBYcZggj44d_"}},{"cell_type":"code","source":["#pivot\n","topic_data[\"topic\"]=topic_data.topic.apply(lambda x: f\"topic_{str(x)}\")\n","topic_data[\"val\"]=int(1)\n","topic_data = pd.pivot(topic_data, index=['user_id', 'created_at', 'id'], columns='topic', values='val')#.reset_index()\n","topic_data.fillna(0, inplace=True)\n","topic_data = pd.DataFrame(topic_data.to_records())\n","topic_data.created_at = pd.to_datetime(topic_data['created_at'], errors='coerce')\n","topic_data.to_csv(f'{data_path}/All_topics_{ntopics}_pivoted_window_{WINDOW}{\"_merged\" if merge else \"\"}.csv')"],"metadata":{"id":"qMFEjEbNuUbg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pij6xTzb8_hQ"},"source":["## Compute Outcomes (2): Topics, and emotions difference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uV0Ihuf-nman"},"outputs":[],"source":["topic_data = pd.read_csv(f'{data_path}/All_topics_{ntopics}_pivoted_window_{WINDOW}{\"_merged\" if merge else \"\"}.csv', index_col =0).reset_index(drop=True)\n","topic_data['created_at'] = pd.to_datetime(topic_data['created_at'], errors='coerce')\n","topic_data.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cSE14Yd79FGc"},"outputs":[],"source":["df = pd.read_csv(f\"{data_path}/tweet_level_data.csv\")\n","df = df.merge(topic_data, on='id', how ='inner', suffixes=['', '_y'])\n","df = df[[col for col in df.columns if  not col.endswith(\"_y\")]]\n","df.created_at = pd.to_datetime(df['created_at'], errors='coerce')\n","df['valence_intensity'] = df.valence_intensity.astype(np.float32)\n","df['anger_intensity'] = df.anger_intensity.astype(np.float32)\n","df['fear_intensity'] = df.fear_intensity.astype(np.float32)\n","df['sadness_intensity'] = df.sadness_intensity.astype(np.float32)\n","df['joy_intensity'] = df.joy_intensity.astype(np.float32)\n","for col in topic_data.columns:\n","  if col.startswith(\"topic_\"):\n","    df[col] = df[col].astype(np.float32)\n","\n","cols =['valence_intensity', 'anger_intensity',\n","         'fear_intensity', 'sadness_intensity', 'joy_intensity',\n","      ] + list(topic_data.columns)\n","df = df[cols]\n","dic = {'user_id':'first', 'valence_intensity':\"mean\", 'anger_intensity':\"mean\", 'fear_intensity':\"mean\",\n","       'sadness_intensity':\"mean\", 'joy_intensity':\"mean\"#pd.Series.mode,\n","       }\n","dic.update({col:'mean' for col in topic_data.columns if col.startswith(\"topic_\")})\n","df.columns\n","#Columns in df:\n","# 'valence_intensity', 'anger_intensity', 'fear_intensity',\n","# 'sadness_intensity', 'joy_intensity', 'user_id', 'created_at', 'id',\n","# 'topic_0', 'topic_1', 'topic_2', 'topic_3'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHD7afxTT8oo"},"outputs":[],"source":["cols = ['valence_intensity', 'anger_intensity',\n","         'fear_intensity', 'sadness_intensity', 'joy_intensity',\n","      ] + list(filter(lambda col:col.startswith(\"topic_\"), topic_data.columns))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e20jfuSxQSMi"},"outputs":[],"source":["for EVENT in EVENTS:\n","  print(EVENT)\n","  eventdate = geteventDate(EVENT)\n","  df[\"diff_days\"] = df['created_at'].apply(lambda x: (eventdate - x).days)\n","\n","  subdata = df[abs(df.diff_days)<=WINDOW]\n","  subdata = subdata[subdata.diff_days>-WINDOW]\n","\n","  print(f\"len data: {len(subdata)}\")\n","  print(f\"num users:{len(subdata.user_id.unique())}\")\n","\n","  post_data = subdata[subdata.diff_days<=0]\n","  pre_data = subdata[subdata.diff_days>0]\n","  print(subdata.columns)\n","\n","  print(f\"post users before aggregating: {len(post_data.user_id.unique())}\")\n","  post_data = post_data.groupby('user_id').agg(dic)\n","  post_data.reset_index(drop = True, inplace = True)\n","  print(f\"post users after:{len(post_data)}\")\n","\n","  print(f\"pre users before aggregating: {len(pre_data.user_id.unique())}\")\n","  pre_data = pre_data.groupby('user_id').agg(dic)\n","  pre_data.reset_index(drop = True, inplace = True)\n","  print(f\"pre users:{len(pre_data)}\")\n","\n","  subdata = pd.merge(post_data, pre_data, on = [\"user_id\"], suffixes=('', '_pre'))\n","\n","  for col in cols:\n","    subdata[f\"{col}_diff\"] = subdata[col] - subdata[f\"{col}_pre\"]\n","    del subdata[col]\n","    del subdata[f\"{col}_pre\"]\n","\n","  subdata.to_csv(f\"{data_path}/{EVENT}_topics{ntopics}_emotions_diff_window_{WINDOW}{'_merged' if merge else ''}.csv\")\n","  print(f\"final length data: {len(subdata)}\")\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"RGbAd2alauCU"},"source":["## Combine pretreatment topics, changes in topics and emotions with rest of data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UkaeDuqFlxCY"},"outputs":[],"source":["ntopics = 7\n","merge = True\n","# topics\n","topic_data = pd.read_csv(f'{data_path}/All_topics_{ntopics}_pivoted_window_{WINDOW}{\"_merged\" if merge else \"\"}.csv', index_col =0).reset_index(drop=True)\n","topic_data['created_at'] = pd.to_datetime(topic_data['created_at'], errors='coerce')\n","dic = {'user_id':'first'}\n","dic.update({col:'mean' for col in topic_data.columns if col.startswith(\"topic_\")})\n","print(topic_data.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ssEUmmoH2twQ"},"outputs":[],"source":["for EVENT in EVENTS:\n","  eventdate = geteventDate(EVENT)\n","  # pre-treatment topics\n","  subtopic_data = topic_data[(topic_data.created_at >= eventdate - timedelta(preTreatmentPeriod))&(topic_data.created_at<eventdate)]\n","  subtopic_data = subtopic_data.groupby('user_id').agg(dic).reset_index(drop=True)\n","\n","  # other pre-treatment covariates, treatment, outcome (change in GCS)\n","  data = pd.read_csv(f\"{data_path}/causal_data_{EVENT}_{str(WINDOW)}_allcovariates_except_topic.csv\", index_col=0)\n","  data = data.merge(subtopic_data, how=\"inner\", on=\"user_id\", suffixes=[\"\", \"_y\"])\n","  data = data.drop(columns=[col for col in data.columns if col.endswith(\"_y\")])\n","\n","  # outcome (2) changes in topics and emotions\n","  diffdata = pd.read_csv(f\"{data_path}/{EVENT}_topics{ntopics}_emotions_diff_window_{WINDOW}{'_merged' if merge else ''}.csv\", index_col=0)\n","  data = data.merge(diffdata, how=\"inner\", on=\"user_id\", suffixes=[\"\", \"_y\"])\n","  data = data.drop(columns=[col for col in data.columns if col.endswith(\"_y\")])\n","\n","  # Save final event-level data: used for TE estimation\n","  data.to_csv(f\"{data_path}/causal_data_{EVENT}_{str(WINDOW)}_allcovariates_topics-{ntopics}{'_merged' if merge else ''}.csv\")"]}],"metadata":{"colab":{"provenance":[{"file_id":"1Lb6auTIbA-2DTZ8K92XHSHlwnPPfgckH","timestamp":1708052183853},{"file_id":"1ldin-7-wW8JcrqJxIfXZM_tKkJ_GBF5M","timestamp":1661552908615}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}